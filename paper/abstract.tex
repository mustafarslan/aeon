\begin{abstract}
Large Language Models (LLMs) are fundamentally constrained by the quadratic computational cost of self-attention and the ``Lost in the Middle'' phenomenon, where reasoning capabilities degrade as context windows expand. Existing solutions, primarily ``Flat RAG'' architectures relying on vector databases, treat memory as an unstructured bag of embeddings. This approach fails to capture the hierarchical and temporal structure of long-horizon interactions, leading to ``Vector Haze''---the retrieval of disjointed facts lacking episodic continuity. We propose \textbf{Aeon}, a Neuro-Symbolic Cognitive Operating System that redefines memory not as a static store, but as a managed OS resource. Aeon structures memory into a \textbf{Memory Palace} (a spatial index implemented via \textsc{Atlas}, a SIMD-accelerated \textbf{Page-Clustered Vector Index} that combines small-world graph navigation with B+ Tree-style disk locality to minimize read amplification) and a \textbf{Trace} (a neuro-symbolic episodic graph). We introduce the \textbf{Semantic Lookaside Buffer (SLB)}, a predictive caching mechanism that exploits conversational locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate that Aeon achieves $< 1$ms retrieval latency on conversational workloads while ensuring state consistency via a zero-copy C++/Python bridge, effectively enabling persistent, structured memory for autonomous agents.
\end{abstract}
