\section{Experimental Methodology}
\label{sec:methodology}

This section describes the experimental setup used to evaluate Aeon's performance characteristics. All experiments were conducted five times, and we report the median value along with the 25th and 75th percentiles to account for variance.

\subsection{Hardware Environment}

We evaluate Aeon on a single Apple M4 Max workstation, representing a high-end consumer-grade ARM64 platform. The system specification is as follows:

\begin{itemize}
    \item \textbf{CPU:} Apple M4 Max, 16-core (12 Performance cores, 4 Efficiency cores), ARM64 architecture.
    \item \textbf{Memory:} 64GB Unified Memory (LPDDR5X) with 546GB/s memory bandwidth.
    \item \textbf{Instruction Set:} ARM NEON SIMD. For portability, AVX-512 equivalence is achieved via the SIMDe translation layer~\cite{simde}, enabling direct compilation of x86 SIMD intrinsics to native ARM instructions.
    \item \textbf{Storage:} 1TB NVMe SSD (Apple internal controller).
\end{itemize}

All native benchmarks were compiled with \texttt{clang-17} using \texttt{-O3 -march=native -flto -ffast-math} optimization flags. Experiments were run under two operating system configurations:
\begin{enumerate}
    \item \textbf{macOS 26.2 (Tahoe):} Native execution for baseline latency measurements.
    \item \textbf{Linux (Debian 12, via Docker):} Containerized execution using Rosetta 2 emulation to evaluate cross-platform deployment scenarios.
\end{enumerate}

To minimize interference, all experiments were run with Efficiency cores disabled, Spotlight indexing paused, and no background applications active. CPU frequency scaling was disabled to ensure consistent clock speeds.

\subsection{Datasets}

We evaluate Aeon using synthetic datasets designed to stress-test the Atlas index under controlled conditions.

\subsubsection{Synthetic Atlas: Dense Forest}
We generate a ``Dense Forest'' of synthetic vectors to simulate a large-scale knowledge base. Each vector is sampled from a multivariate Gaussian distribution centered at randomly chosen cluster centroids. Dataset sizes are varied across three orders of magnitude:

\begin{itemize}
    \item $N = 10^4$ nodes (small, fits entirely in L3 cache).
    \item $N = 10^5$ nodes (medium, representative of a substantial personal knowledge base).
    \item $N = 10^6$ nodes (large, simulates enterprise-scale deployments).
\end{itemize}

All vectors have dimensionality $D = 768$, matching the embedding dimension of widely-used models such as BERT~\cite{devlin2019bert} and Llama-2~\cite{touvron2023llama}.

\subsubsection{Workload Traces}
We define two distinct workload profiles to simulate real-world query patterns:

\begin{enumerate}
    \item \textbf{Uniform Random:} Query vectors are sampled uniformly at random from the embedding space. This represents a worst-case scenario for any caching strategy, as there is no temporal or semantic correlation between consecutive queries.
    \item \textbf{Conversational Walk:} Query vectors are generated by performing a random walk on the semantic graph. Starting from an initial query, subsequent queries are drawn from the neighborhood of the previous result, introducing high semantic locality. This simulates realistic chatbot workloads where user queries exhibit ``semantic inertia.''
\end{enumerate}

\subsection{Baselines}

We compare Aeon against two baseline systems representing the spectrum from naive to state-of-the-art approaches:

\begin{itemize}
    \item \textbf{Baseline A (Flat Search):} A brute-force linear scan over all $N$ vectors. Similarity is computed using a vectorized dot product kernel. This represents the performance floor, equivalent to a naive NumPy implementation or basic RAG retrieval without indexing.
    \item \textbf{Baseline B (HNSW):} Hierarchical Navigable Small World graph~\cite{malkov2018hnsw}, the de facto industry standard for approximate nearest neighbor search. We use the FAISS~\cite{johnson2019faiss} implementation with default parameters (\texttt{M=32}, \texttt{efConstruction=200}, \texttt{efSearch=64}).
\end{itemize}

We evaluate two configurations of Aeon to isolate the contribution of the Semantic Lookaside Buffer (SLB):

\begin{itemize}
    \item \textbf{Aeon (Cold):} Atlas search with the SLB disabled. The search always starts from the root node.
    \item \textbf{Aeon (Warm):} Atlas search with the SLB enabled. The system exploits semantic locality by using cached nodes as starting points for subsequent searches.
\end{itemize}

\subsection{Metrics}

We report the following metrics to provide a comprehensive performance profile:

\begin{itemize}
    \item \textbf{P99 Latency (ms):} The 99th percentile latency for a single query. Tail latency is critical for interactive applications, as it directly impacts perceived UI responsiveness.
    \item \textbf{QPS (Queries Per Second):} Throughput measured under sustained load. We report peak QPS achieved when queries are issued in a tight loop with no artificial delays.
    \item \textbf{Memory Footprint (MB):} Resident Set Size (RSS) as measured by the \texttt{/proc/[pid]/statm} interface on Linux, or the \texttt{footprint} metric from \texttt{libproc} on macOS.
    \item \textbf{Cache Hit Rate (\%):} For the Warm configuration, the percentage of queries where the SLB provided a valid starting node closer to the target than the root node. A hit is defined as $\text{sim}(q, n_{\text{slb}}) > \text{sim}(q, n_{\text{root}})$.
\end{itemize}

All latency measurements are taken using \texttt{std::chrono::high\_resolution\_clock} with nanosecond precision. We exclude the first 100 queries from each run to allow the system to reach a steady state (warm caches, JIT compilation for Python components).
